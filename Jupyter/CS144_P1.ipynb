{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw2VLbV7RED_"
      },
      "source": [
        "Notes:\n",
        "there are sites that stores the records for humidity, precipitation, wind speed, temperature of a specific city. If we want to add those features to increase reliability and validity of our model, we should be able to do it manually or maybe someone can figure out a way to properly data scrape / data mine. We can even add a boolean for whether it rained during that timeframe but that is more precise.\n",
        "\n",
        "*   https://www.timeanddate.com/weather/philippines/makati/historic\n",
        "*   https://www.wunderground.com/history/daily/ph/makati/IMAKAT1/date/2022-6-5\n",
        "*   https://weatherspark.com/m/136799/2/Average-Weather-in-February-in-Makati-City-Philippines\n",
        "*   https://www.accuweather.com/en/ph/makati-city/264878/february-weather/264878\n",
        "\n",
        "For the incoming paper, here's a good reference:\n",
        "https://scholarworks.rit.edu/cgi/viewcontent.cgi?article=12537&context=theses\n",
        "<br>\n",
        "Relevant but not for the paper:\n",
        "https://www.sciencedirect.com/science/article/abs/pii/S0379711218303941"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHVRiaUwJQ5A"
      },
      "source": [
        "Goal:\n",
        "\n",
        "Model with at least 60, but ideally 80% accuracy.\n",
        "Right now, the status quo is that ohe is not enough, at least for basic models. we could also initially test our original df na formatted na into categorical into XDG models that allows categorical data for input but I don't expect much."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti7A4ToqNlZX"
      },
      "source": [
        "#  Data Preparation and Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOldPwSt6wer",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6f084a9-392f-42e8-dd6d-35241a159445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ydata_profiling\n",
            "  Downloading ydata_profiling-4.6.5-py2.py3-none-any.whl (357 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m357.9/357.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy<1.12,>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ydata_profiling) (1.11.4)\n",
            "Requirement already satisfied: pandas!=1.4.0,<3,>1.1 in /usr/local/lib/python3.10/dist-packages (from ydata_profiling) (1.5.3)\n",
            "Requirement already satisfied: matplotlib<3.9,>=3.2 in /usr/local/lib/python3.10/dist-packages (from ydata_profiling) (3.7.1)\n",
            "Requirement already satisfied: pydantic>=2 in /usr/local/lib/python3.10/dist-packages (from ydata_profiling) (2.6.3)\n",
            "Requirement already satisfied: PyYAML<6.1,>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from ydata_profiling) (6.0.1)\n",
            "Requirement already satisfied: jinja2<3.2,>=2.11.1 in /usr/local/lib/python3.10/dist-packages (from ydata_profiling) (3.1.3)\n",
            "Collecting visions[type_image_path]==0.7.5 (from ydata_profiling)\n",
            "  Downloading visions-0.7.5-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.7/102.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.26,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ydata_profiling) (1.25.2)\n",
            "Collecting htmlmin==0.1.12 (from ydata_profiling)\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting phik<0.13,>=0.11.1 (from ydata_profiling)\n",
            "  Downloading phik-0.12.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (686 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m686.1/686.1 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.24.0 in /usr/local/lib/python3.10/dist-packages (from ydata_profiling) (2.31.0)\n",
            "Requirement already satisfied: tqdm<5,>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from ydata_profiling) (4.66.2)\n",
            "Collecting seaborn<0.13,>=0.10.1 (from ydata_profiling)\n",
            "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m293.3/293.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multimethod<2,>=1.4 (from ydata_profiling)\n",
            "  Downloading multimethod-1.11.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: statsmodels<1,>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from ydata_profiling) (0.14.1)\n",
            "Collecting typeguard<5,>=4.1.2 (from ydata_profiling)\n",
            "  Downloading typeguard-4.1.5-py3-none-any.whl (34 kB)\n",
            "Collecting imagehash==4.3.1 (from ydata_profiling)\n",
            "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wordcloud>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from ydata_profiling) (1.9.3)\n",
            "Collecting dacite>=1.8 (from ydata_profiling)\n",
            "  Downloading dacite-1.8.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba<0.59.0,>=0.56.0 in /usr/local/lib/python3.10/dist-packages (from ydata_profiling) (0.58.1)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.10/dist-packages (from imagehash==4.3.1->ydata_profiling) (1.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from imagehash==4.3.1->ydata_profiling) (9.4.0)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]==0.7.5->ydata_profiling) (23.2.0)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.10/dist-packages (from visions[type_image_path]==0.7.5->ydata_profiling) (3.2.1)\n",
            "Collecting tangled-up-in-unicode>=0.0.4 (from visions[type_image_path]==0.7.5->ydata_profiling)\n",
            "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2<3.2,>=2.11.1->ydata_profiling) (2.1.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (23.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib<3.9,>=3.2->ydata_profiling) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba<0.59.0,>=0.56.0->ydata_profiling) (0.41.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas!=1.4.0,<3,>1.1->ydata_profiling) (2023.4)\n",
            "Requirement already satisfied: joblib>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from phik<0.13,>=0.11.1->ydata_profiling) (1.3.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ydata_profiling) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ydata_profiling) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ydata_profiling) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.24.0->ydata_profiling) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.24.0->ydata_profiling) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.24.0->ydata_profiling) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.24.0->ydata_profiling) (2024.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.4 in /usr/local/lib/python3.10/dist-packages (from statsmodels<1,>=0.13.2->ydata_profiling) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.4->statsmodels<1,>=0.13.2->ydata_profiling) (1.16.0)\n",
            "Building wheels for collected packages: htmlmin\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27081 sha256=e84b6e099b29d1be4fb48a1496889cd767b571f79bdfa804f96c9c49d52a766e\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/91/29/a79cecb328d01739e64017b6fb9a1ab9d8cb1853098ec5966d\n",
            "Successfully built htmlmin\n",
            "Installing collected packages: htmlmin, typeguard, tangled-up-in-unicode, multimethod, dacite, imagehash, visions, seaborn, phik, ydata_profiling\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.1\n",
            "    Uninstalling seaborn-0.13.1:\n",
            "      Successfully uninstalled seaborn-0.13.1\n",
            "Successfully installed dacite-1.8.1 htmlmin-0.1.12 imagehash-4.3.1 multimethod-1.11.2 phik-0.12.4 seaborn-0.12.2 tangled-up-in-unicode-0.2.0 typeguard-4.1.5 visions-0.7.5 ydata_profiling-4.6.5\n"
          ]
        }
      ],
      "source": [
        "!pip install ydata_profiling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9KVSuFykwcBf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "#from ydata_profiling import ProfileReport #one liner eda, not natively installed in google colab so do !pip install ydata_profiling or just remove this.\n",
        "from sklearn import linear_model #I2\n",
        "from sklearn.preprocessing import OneHotEncoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "id": "cuQFaRNmmFTs",
        "outputId": "2f2e3fda-147d-4abc-9dcd-aae84d22becd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "HTTP Error 404: Not Found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-43eddc09c171>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Unfortunately for the case of location and i/e/e/o, the data is too disorganized to form some logic without too much complexity or nested if' statements, so we resort to manually adjusting the values into a uniform and standardized format.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mURL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://raw.githubusercontent.com/Vz-3/dataset_storage/master/Formatted-Firesight-Dataset.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mURL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m     \u001b[0;31m# open URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m     ioargs = _get_filepath_or_buffer(\n\u001b[0m\u001b[1;32m    714\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m         \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36m_get_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;31m# assuming storage_options is to be interpreted as headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mreq_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq_info\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Content-Encoding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# request was successfully received, understood, and accepted.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             response = self.parent.error(\n\u001b[0m\u001b[1;32m    635\u001b[0m                 'http', request, response, code, msg, hdrs)\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
          ]
        }
      ],
      "source": [
        "#Unfortunately for the case of location and i/e/e/o, the data is too disorganized to form some logic without too much complexity or nested if' statements, so we resort to manually adjusting the values into a uniform and standardized format.\n",
        "URL = 'https://raw.githubusercontent.com/Vz-3/dataset_storage/master/Formatted-Firesight-Dataset.csv'\n",
        "df = pd.read_csv(URL)\n",
        "\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSaXKt2ZmxU3"
      },
      "source": [
        "<h3> Fundamentals </hr>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoYoRlvlnPjx"
      },
      "outputs": [],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G360RD1Wmhdk"
      },
      "outputs": [],
      "source": [
        "df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KgBGaSNqAZd"
      },
      "outputs": [],
      "source": [
        "df.info() #does df.dtypes, df.column, df.isna().sum() inverse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQsFMkKDFKE6"
      },
      "outputs": [],
      "source": [
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYRzkR2lsazo"
      },
      "outputs": [],
      "source": [
        "df.duplicated()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLPavWej8WRt"
      },
      "outputs": [],
      "source": [
        "df.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LK2KS6dm2sx"
      },
      "source": [
        "<h3> Cleaning </h3>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46_xvLBG9QXT"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#Ignore this (had to reformat the day again since it has multiple redundancies of format, as shown by the next line of code) [unless if you want this to be added in cleansing]\n",
        "print(df['DOTW'].value_counts())\n",
        "print('Old:',df['DOTW'].value_counts().sum())\n",
        "dayFormat = ['su','m','tu','w','th', 'f', 'sa']\n",
        "\n",
        "toAdjust = [\n",
        "    (df['DOTW'].isin(dayFormat)),\n",
        "    (df['DOTW'] == 't') | (df['DOTW'] == 'tues'),\n",
        "    (df['DOTW'] == 'sat') | (df['DOTW'] == 's'),\n",
        "    (df['DOTW'] == 'sun')\n",
        "]\n",
        "\n",
        "values = [df['DOTW'],'tu','sa', 'su']\n",
        "df['DOTW'] = np.select(toAdjust, values, df['DOTW']) #params: list of conditions, values for each condition, default value\n",
        "df['DOTW'].astype('category')\n",
        "print('New:',df['DOTW'].value_counts().sum())\n",
        "\n",
        "#for brevity and readability\n",
        "renameTo = ['DOTW','DATETIME','BRGY','ENTITIES','FIRE_CAUSE/ORIGIN','ALARM_LEVEL','DEATHS','INJURIES','RESPONDERS','ESTIMATED_PHP_LOSS']\n",
        "\n",
        "df.rename(columns={col: new_name for col, new_name in zip(df.columns, renameTo)}, inplace=True)\n",
        "df.columns\n",
        "\n",
        "df['HOUR'] = df['DATETIME'].apply(lambda text: text[12:14])\n",
        "df['DATETIME'] = df['DATETIME'].apply(lambda text: text[0:6])\n",
        "df[['DAY','MONTH']] = df['DATETIME'].apply(lambda text: pd.Series(text.split(\" \")))\n",
        "df.drop(['DATETIME'],axis=1, inplace=True)\n",
        "\n",
        "#Some minor issues in HOUR unique count so we turn them into integers to address the issue by first checking with pd.to_numeric(df['HOUR'], errors='coerce').notnull().all() to see if all the contents of the column is a numeric\n",
        "df['HOUR'] = df['HOUR'].astype(int)\n",
        "\n",
        "df.head()\n",
        "\n",
        "#Now if we want to go back to a single column but with the datatype of datetime for time-series data with the loss of minutes,  we can do this by:\n",
        "#there are a lot of benefits for converting numerical dates into pandas datetime, but IDK if we can properly utilize it since we only have 1 year and time-series requires a lot of data, that's why historical data is a big part of big data.\n",
        "df['DATETIME'] = pd.to_datetime(\n",
        "    df['MONTH'] + ' ' + df['DAY'].astype(str) + ', 2023 ' + df['HOUR'].astype(str) + ':00:00'\n",
        ")\n",
        "#delete the other columns\n",
        "df.drop(columns=['HOUR','DAY','MONTH'],inplace=True)\n",
        "#rearrange again\n",
        "df = df[renameTo]\n",
        "df.head()\n",
        "\n",
        "print(f\"Initial: {df['FIRE_CAUSE/ORIGIN'].nunique()}\")\n",
        "\n",
        "df['FIRE_CAUSE'] = df['FIRE_CAUSE/ORIGIN'].apply(lambda x: re.sub(r'((.*(?<=caused...)\\s*)|(\\s(?=cause:).*))|(.*(?<=from).)','',x) if ((\"caused\" in x or \"cause:\" in x) or \"from\" in x) else x)\n",
        "\n",
        "print(f'New: {df.FIRE_CAUSE.nunique()}')\n",
        "df['FIRE_CAUSE'].head()\n",
        "\n",
        "df['FIRE_ORIGIN'] = df['FIRE_CAUSE/ORIGIN'].apply(lambda x: re.sub(r'((\\s(?=caused...).*)|(\\s(?=cause:).*))|(\\s(?=from).*)','',x) if ((\"caused\" in x or \"cause:\" in x) or \"from\" in x) else x)\n",
        "print(f'New: {df.FIRE_ORIGIN.nunique()}')\n",
        "df['FIRE_ORIGIN'].head()\n",
        "\n",
        "df['ESTIMATED_PHP_LOSS'] = df['ESTIMATED_PHP_LOSS'].apply(lambda val: int(float(re.sub('(php|,)','',val))))\n",
        "df['ESTIMATED_PHP_LOSS'].head()\n",
        "\n",
        "#for this case since there are some missing values, let's address them by replacing them with Negative.\n",
        "df[['DEATHS','INJURIES']] = df[['DEATHS','INJURIES']].fillna('negative')\n",
        "df.isna().sum()\n",
        "\n",
        "df[['DEATHS','INJURIES']] = df[['DEATHS','INJURIES']].applymap(lambda foo: False if foo == 'negative' else True)\n",
        "df.head(10)\n",
        "\n",
        "df.nunique()\n",
        "df[['DEATHS','INJURIES']] = df[['DEATHS','INJURIES']].astype('boolean')\n",
        "\n",
        "df.drop(['RESPONDERS'], axis=1)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5ILOtwNJ917"
      },
      "outputs": [],
      "source": [
        "#It's cleaner and easier if we ignore case by turning all values into lowercase as well as removing extra spaces.\n",
        "df = df.applymap(lambda text: re.sub(' +',' ',text.lower()) if isinstance(text, str) else text)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQtsvaQ10auC"
      },
      "outputs": [],
      "source": [
        "df['ENTITIES'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw3KV7PH2GVP"
      },
      "outputs": [],
      "source": [
        "#Let's remove the special characters to properly reflect the uniqueness of Entities.\n",
        "df['ENTITIES'] = df['ENTITIES'].apply(lambda x: re.sub('_|-','',x))\n",
        "df['ENTITIES'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84dPo-Sl0eN6"
      },
      "outputs": [],
      "source": [
        "df['FIRE_ORIGIN'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fn8N5TNR0yGD"
      },
      "outputs": [],
      "source": [
        "df['FIRE_CAUSE'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Vt06Yj7ti5Y"
      },
      "outputs": [],
      "source": [
        "df['ALARM_LEVEL'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DvSPqvtP4jB"
      },
      "outputs": [],
      "source": [
        "#df['ALARM_LEVEL'] = df['ALARM_LEVEL'].fillna('missing')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ8N-tizgzlx"
      },
      "outputs": [],
      "source": [
        "#Converting into proper data types.\n",
        "df['DOTW'] = df['DOTW'].astype('category')\n",
        "df['MONTH'] = df['MONTH'].astype('category')\n",
        "df['DAY'] = df['DAY'].astype('int')\n",
        "df['ENTITIES'] = df['ENTITIES'].astype('category')\n",
        "df['BRGY'] = df['BRGY'].astype('category')\n",
        "df['FIRE_CAUSE'] = df['FIRE_CAUSE'].astype('category')\n",
        "df['FIRE_ORIGIN'] = df['FIRE_ORIGIN'].astype('category')\n",
        "\n",
        "df['ALARM_LEVEL'] = df['ALARM_LEVEL'].astype('category')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxpgYlbxIEfk"
      },
      "outputs": [],
      "source": [
        "#Reordering our columns for readability\n",
        "df = df[['DOTW','DAY','HOUR','MONTH','BRGY','ENTITIES','FIRE_CAUSE','FIRE_ORIGIN','ALARM_LEVEL','DEATHS','INJURIES','ESTIMATED_PHP_LOSS']]\n",
        "#Uncomment this if you want to download the formatted dataset once it's cleaned.\n",
        "#df.to_csv('Formatted-Firesight-Dataset.csv',index=False)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfs5Brp9NpV1"
      },
      "source": [
        "# Exploratory Data Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuCkY00fbA6i"
      },
      "outputs": [],
      "source": [
        "# profiler = ProfileReport(df, title=\"Firesight Profiling Report\")\n",
        "# profiler.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jny6ud37n0P4"
      },
      "outputs": [],
      "source": [
        "#try ko lang for initial visualization - (thea)\n",
        "#it's actually advised that for EDA, we use tools like PowerBI or even webpivottable to help us visualize and derive analysis and insights -v, also no need to reimport the packages, yung mga bago lang."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXGzf7ajvmKP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import missingno as msno\n",
        "import plotly.express as px\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p5krUeajD_4a"
      },
      "outputs": [],
      "source": [
        "msno.matrix(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYgkLat4n6o0"
      },
      "outputs": [],
      "source": [
        "df.replace(np.NaN,0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duw6gYqorZ5J"
      },
      "outputs": [],
      "source": [
        "entities_counts = df['ENTITIES'].value_counts()\n",
        "top_entities = entities_counts.head(5)\n",
        "labels = top_entities.index\n",
        "counts = top_entities.values\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "plt.axis('equal')\n",
        "plt.title(\"TOP 5 INVOLVED ESTABLISHMENTS IN FIRE CASES - MAKATI CITY 2022\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-8xoFlrn_Bv"
      },
      "outputs": [],
      "source": [
        "causes_counts = df['FIRE_ORIGIN'].value_counts()\n",
        "top_causes = causes_counts.head(5)\n",
        "labels = top_causes.index\n",
        "counts = top_causes.values\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.pie(counts, labels=labels, autopct='%1.1f%%', startangle=140)\n",
        "plt.axis('equal')\n",
        "plt.title(\"TOP 5 ORIGIN OF FIRE CASES - MAKATI CITY 2022\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q01aPqBHPLdO"
      },
      "source": [
        "# Setup (Preprocessing, Balancing, etc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUTky23dVJAD"
      },
      "outputs": [],
      "source": [
        "tempDf = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpOw-fj-PdoN"
      },
      "outputs": [],
      "source": [
        "#Drop deaths and injuries\n",
        "tempDf.drop(['DOTW','DEATHS','INJURIES', 'FIRE_CAUSE'], axis=1,inplace=True)\n",
        "tempDf.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qicucdMonKDN"
      },
      "outputs": [],
      "source": [
        "labelEncodeDf = tempDf.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COoKx8Q4nUMz"
      },
      "outputs": [],
      "source": [
        "#All label encode test\n",
        "labelEncodeDf = labelEncodeDf.apply(lambda col: col.astype('category').cat.codes if col.dtype == 'category' else col)\n",
        "labelEncodeDf.info()\n",
        "\n",
        "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "\n",
        "y = labelEncodeDf['ESTIMATED_PHP_LOSS']\n",
        "X = labelEncodeDf.loc[:, labelEncodeDf.columns != 'ESTIMATED_PHP_LOSS']\n",
        "\n",
        "#Baseline model\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "\n",
        "lasso = Lasso()\n",
        "lr = LinearRegression()\n",
        "ridge = Ridge(alpha=1.0)\n",
        "\n",
        "lasso.fit(X_train, y_train)\n",
        "print(f'MAE: {mean_absolute_error(y_test,lasso.predict(X_test))}')\n",
        "print(f'MSE: {mean_squared_error(y_test,lasso.predict(X_test))}')\n",
        "print('Score: ',lasso.score(X_test, y_test))\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "print(f'MAE: {mean_absolute_error(y_test,lr.predict(X_test))}')\n",
        "print(f'MSE: {mean_squared_error(y_test,lr.predict(X_test))}')\n",
        "print('Score: ',lr.score(X_test, y_test))\n",
        "\n",
        "ridge.fit(X_train, y_train)\n",
        "print(f'MAE: {mean_absolute_error(y_test,ridge.predict(X_test))}')\n",
        "print(f'MSE: {mean_squared_error(y_test,ridge.predict(X_test))}')\n",
        "print('Score: ',ridge.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8r_fHgD-4Kua"
      },
      "outputs": [],
      "source": [
        "# profiler = ProfileReport(labelEncodeDf,title=\"Firesight Profiling Report: LE\")\n",
        "# profiler.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FuCrqx1hbbWF"
      },
      "outputs": [],
      "source": [
        "tempDf.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8gWjLDyVZfZ"
      },
      "outputs": [],
      "source": [
        "tempDf.nunique()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed8-d1ffgLxh"
      },
      "source": [
        "BRGY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYYab7xPg4cL"
      },
      "outputs": [],
      "source": [
        "districtTwo = ['cembo','comembo','east rembo','guadalupe nuevo','guadalupe viejo','pembo','pinagkaisahan','pitogo','southside palar','rizal','south cembo','west rembo']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpW-edLagzgT"
      },
      "outputs": [],
      "source": [
        "tempDf['DISTRICT1'] = tempDf['BRGY'].apply(lambda brgy: 0 if brgy in districtTwo else 1) #direct encoding (because it's boolean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu_GZqQ_lu3-"
      },
      "outputs": [],
      "source": [
        "tempDf.drop('BRGY',axis=1,inplace=True)\n",
        "tempDf.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "av51Y79sy-Y3"
      },
      "source": [
        "FIRE_ORIGIN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SCk6RPyRzBoX"
      },
      "outputs": [],
      "source": [
        "frequentOrigin = ['electrical ignition', 'open flame', 'lighted cigarette', 'overheated engine', 'ignition of materials', 'battery short circuit']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cCtc7tg0zfNT"
      },
      "outputs": [],
      "source": [
        "tempDf['FIRE_ORIGIN'] = tempDf['FIRE_ORIGIN'].apply(lambda fire_origin: fire_origin if fire_origin in frequentOrigin else 'others')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xownvfdv2FA6"
      },
      "outputs": [],
      "source": [
        "tempDf['FIRE_ORIGIN'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVjN69yDbZQq"
      },
      "outputs": [],
      "source": [
        "#Strategies\n",
        "#MONTH - nominal data, likely just label encode\n",
        "#BRGY - must be label encode or target encode (Otherwise generalize into district 1 and district 2 only)\n",
        "#ENTITIES - can onehotencode or target\n",
        "#Fire_Cause - delete\n",
        "#Fire_Origin -"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encDf = tempDf.copy()\n",
        "encDf.info()\n",
        "tempDf.info()"
      ],
      "metadata": {
        "id": "SBkTBnX1JTae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xnc4fn35zEJO"
      },
      "outputs": [],
      "source": [
        "ohe = OneHotEncoder()\n",
        "result = 0\n",
        "tempDf['FIRE_ORIGIN'] = tempDf['FIRE_ORIGIN'].astype('category')\n",
        "for col in tempDf.columns:\n",
        "    if col == 'MONTH' or col == 'ALARM_LEVEL':\n",
        "        continue\n",
        "    else:\n",
        "        print(col)\n",
        "        if tempDf[col].dtypes == 'category':\n",
        "            encData = ohe.fit_transform(tempDf[[col]]).toarray()\n",
        "            newdf = pd.DataFrame(encData, columns=ohe.get_feature_names_out())\n",
        "            encDf = pd.concat([encDf,newdf], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encDf = encDf.drop(['MONTH','FIRE_ORIGIN','ENTITIES'],axis=1)\n",
        "encDf.head()"
      ],
      "metadata": {
        "id": "yn8S81KNIA4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwoNTz6K345v"
      },
      "outputs": [],
      "source": [
        "#profiler = ProfileReport(encDf,title=\"Firesight Profiling Report: OHE\")\n",
        "#profiler.to_notebook_iframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuXIMt0K2p94"
      },
      "outputs": [],
      "source": [
        "encDf.nunique()\n",
        "encDf.info()\n",
        "encDf['ESTIMATED_PHP_LOSS'].value_counts()\n",
        "encDf.to_csv('dataset.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IABGHw526bT"
      },
      "outputs": [],
      "source": [
        "#Baseline?\n",
        "ye = encDf['ESTIMATED_PHP_LOSS']\n",
        "Xe = encDf.loc[:, encDf.columns != 'ESTIMATED_PHP_LOSS']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(Xe, ye, test_size=0.20)\n",
        "\n",
        "lasso = Lasso()\n",
        "lr = LinearRegression()\n",
        "ridge = Ridge(alpha=1.0)\n",
        "\n",
        "lasso.fit(X_train, y_train)\n",
        "print(f'MAE: {mean_absolute_error(y_test,lasso.predict(X_test))}')\n",
        "print(f'MSE: {mean_squared_error(y_test,lasso.predict(X_test))}')\n",
        "print('Score: ',lasso.score(X_test, y_test))\n",
        "\n",
        "lr.fit(X_train, y_train)\n",
        "print(f'MAE: {mean_absolute_error(y_test,lr.predict(X_test))}')\n",
        "print(f'MSE: {mean_squared_error(y_test,lr.predict(X_test))}')\n",
        "print('Score: ',lr.score(X_test, y_test))\n",
        "\n",
        "ridge.fit(X_train, y_train)\n",
        "print(f'MAE: {mean_absolute_error(y_test,ridge.predict(X_test))}')\n",
        "print(f'MSE: {mean_squared_error(y_test,ridge.predict(X_test))}')\n",
        "print('Score: ',ridge.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_6BIZKEVXoi"
      },
      "source": [
        "# Data Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regression:"
      ],
      "metadata": {
        "id": "aeytSJiXMi63"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8PWIm7tVsLI"
      },
      "outputs": [],
      "source": [
        "encDf.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W1Z2aG7tu728"
      },
      "outputs": [],
      "source": [
        "def z_score_standardization(encDf):\n",
        "    X_mean = np.mean(encDf)\n",
        "    X_std = np.std(encDf)\n",
        "    X_standardized = (encDf - X_mean) / X_std\n",
        "    return X_standardized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl2NUsyIvFoP"
      },
      "outputs": [],
      "source": [
        "# Applying Z-Score Standardization\n",
        "encDf[['DAY','HOUR']] = z_score_standardization(encDf[['DAY','HOUR']])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "y_nn = encDf[['ESTIMATED_PHP_LOSS']]\n",
        "X_nn = encDf.drop('ESTIMATED_PHP_LOSS',axis=1)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_nn, y_nn, test_size=0.33, random_state=1)\n",
        "\n",
        "nn = MLPRegressor(\n",
        "    activation='relu',\n",
        "    hidden_layer_sizes=(10, 100),\n",
        "    alpha=0.001,\n",
        "    random_state=20,\n",
        "    early_stopping=False\n",
        ")\n",
        "\n",
        "nn.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "pred = nn.predict(X_test)\n",
        "test_set_rsquared = nn.score(X_test, y_test)\n",
        "test_set_rmse = np.sqrt(mean_squared_error(y_test, pred))\n",
        "\n",
        "print('R_squared value: ', test_set_rsquared)\n",
        "print('RMSE: ', test_set_rmse)"
      ],
      "metadata": {
        "id": "EjmEtVev_N_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxX-zddcv9EQ"
      },
      "outputs": [],
      "source": [
        "#Data preprocessing\n",
        "#X = X_standardized.drop(\"ESTIMATED_PHP_LOSS\", axis=1)\n",
        "#y = X_standardized[\"ESTIMATED_PHP_LOSS\"]\n",
        "\n",
        "X = encDf.drop('ESTIMATED_PHP_LOSS',axis=1)\n",
        "y = encDf[\"ESTIMATED_PHP_LOSS\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5jBLKLlxx-V"
      },
      "source": [
        "Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCFuaPELx6rc"
      },
      "outputs": [],
      "source": [
        "# training model\n",
        "model_linreg = LinearRegression().fit(X_train, y_train)\n",
        "\n",
        "# predicting the test set results\n",
        "linreg_pred = model_linreg.predict(X_test)\n",
        "\n",
        "#calculating the mean squared error\n",
        "mse = mean_squared_error(y_test, linreg_pred)\n",
        "\n",
        "#calculating the root mean squared error\n",
        "rmse = mse ** 0.5\n",
        "\n",
        "#calculating the r2 score\n",
        "r2score = r2_score(y_test, linreg_pred)\n",
        "\n",
        "print('Mean: ', np.mean(encDf['ESTIMATED_PHP_LOSS']))\n",
        "print('Median: ', np.median(encDf['ESTIMATED_PHP_LOSS']))\n",
        "print('Mean Squared Error: ', mse)\n",
        "print('Root Mean Squared Error:', rmse)\n",
        "print('R2 Score:', r2score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkvE8gEi9CTq"
      },
      "source": [
        "Polynomial Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AS28FGO813z"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# Polynomial Regression\n",
        "degree = 3\n",
        "poly_features = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly_features.fit_transform(X_train)\n",
        "X_test_poly = poly_features.transform(X_test)\n",
        "poly_reg_model = LinearRegression()\n",
        "poly_reg_model.fit(X_train_poly, y_train)\n",
        "y_pred_poly = poly_reg_model.predict(X_test_poly)\n",
        "\n",
        "# Evaluation\n",
        "mse_poly = mean_squared_error(y_test, y_pred_poly)\n",
        "r2_poly = r2_score(y_test, y_pred_poly)\n",
        "print(\"Polynomial Regression (Degree {}) - MSE: {:.4f}, R2: {:.4f}\".format(degree, mse_poly, r2_poly))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rCJ-UWV9Msh"
      },
      "source": [
        "Decision Tree Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N1f2U2ln9PyD"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "# Decision Tree Regression\n",
        "tree_reg_model = DecisionTreeRegressor()\n",
        "tree_reg_model.fit(X_train, y_train)\n",
        "y_pred_tree = tree_reg_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse_tree = mean_squared_error(y_test, y_pred_tree)\n",
        "r2_tree = r2_score(y_test, y_pred_tree)\n",
        "print(\"Decision Tree Regression - MSE: {:.4f}, R2: {:.4f}\".format(mse_tree, r2_tree))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbCZtpym91ky"
      },
      "source": [
        "Gradient Boosting Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OROofV195GL"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Gradient Boosting Regression\n",
        "gb_reg_model = GradientBoostingRegressor()\n",
        "gb_reg_model.fit(X_train, y_train)\n",
        "y_pred_gb = gb_reg_model.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "mse_gb = mean_squared_error(y_test, y_pred_gb)\n",
        "r2_gb = r2_score(y_test, y_pred_gb)\n",
        "print(\"Gradient Boosting Regression - MSE: {:.4f}, R2: {:.4f}\".format(mse_gb, r2_gb))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classification"
      ],
      "metadata": {
        "id": "Q_XojPSIMocr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clfDf = encDf.copy()\n",
        "clfDf = clfDf.drop(['DAY','HOUR'], axis=1)\n",
        "clfDf['ESTIMATED_PHP_LOSS'] = clfDf['ESTIMATED_PHP_LOSS'].apply(lambda val: 'low' if val < 100000 else ('medium' if val < 1000000 else 'high'))\n",
        "clfDf['ESTIMATED_PHP_LOSS'].head()"
      ],
      "metadata": {
        "id": "rvKSVwsEMnk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dealing with imbalance using oversampling and undersampling"
      ],
      "metadata": {
        "id": "WLhmoOn7mjGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clfDf['ESTIMATED_PHP_LOSS'].value_counts()\n",
        "X = clfDf.drop('ESTIMATED_PHP_LOSS',axis=1)\n",
        "y = clfDf['ESTIMATED_PHP_LOSS']"
      ],
      "metadata": {
        "id": "Z-EbaCBkNB-L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "\n",
        "resample_pipeline = Pipeline([\n",
        "    ('oversample', SMOTE()),  # Increase the number of samples in the minority class to 50% of the majority class\n",
        "    ('undersample', RandomUnderSampler()),  # Reduce the number of samples in the majority class to 80% of the minority class\n",
        "])\n",
        "\n",
        "X_resampled, y_resampled = resample_pipeline.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "rQ6BOt0FmmO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.33, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=11)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "knnPred = knn.predict(X_test)"
      ],
      "metadata": {
        "id": "AbP4kn3bNKtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fzah7_VKVuMX"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "class_labels = ['high', 'low', 'medium']\n",
        "print(\"CL Report: \",classification_report(y_test, knnPred, zero_division=1))\n",
        "confusionMatrix = confusion_matrix(y_test, knnPred, labels=class_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusionMatrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=class_labels,\n",
        "            yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = clfDf[['FIRE_ORIGIN_battery short circuit', 'FIRE_ORIGIN_electrical ignition',\n",
        "       'FIRE_ORIGIN_ignition of materials', 'FIRE_ORIGIN_lighted cigarette',\n",
        "       'FIRE_ORIGIN_open flame', 'FIRE_ORIGIN_others',\n",
        "       'FIRE_ORIGIN_overheated engine']]\n",
        "y = clfDf['ESTIMATED_PHP_LOSS']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=23)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=9)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "knnPred2 = knn.predict(X_test)\n",
        "print(\"CL Report: \",classification_report(y_test, knnPred2, zero_division=1))\n",
        "confusionMatrix = confusion_matrix(y_test, knnPred2, labels=class_labels)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(confusionMatrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=class_labels,\n",
        "            yticklabels=class_labels)\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wNHqUCv4Yfwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CLASSIFICATION + SIMULATION"
      ],
      "metadata": {
        "id": "L0AJOtNesppN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "last updated: aug 1 12:05 AM - vash n teya"
      ],
      "metadata": {
        "id": "ukZP98_0ss4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clfDf['ESTIMATED_PHP_LOSS'].value_counts() #target"
      ],
      "metadata": {
        "id": "u5lqnLRDspBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# X is the predictor and y is the target variable\n",
        "X = tempDf[['FIRE_ORIGIN']]\n",
        "y = clfDf['ESTIMATED_PHP_LOSS']\n",
        "\n",
        "# Perform label encoding on the categorical features\n",
        "label_encoder_X = LabelEncoder()\n",
        "X_encoded = X.apply(label_encoder_X.fit_transform)\n",
        "\n",
        "label_encoder_y = LabelEncoder()\n",
        "y_encoded = label_encoder_y.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
        "# Create a logistic regression model\n",
        "model = LogisticRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "id": "Ib3CS6J_sxoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample test data for simulation (replace this with your actual test data)\n",
        "test_data = tempDf['FIRE_ORIGIN']\n",
        "\n",
        "# Convert the test data to DataFrame\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "# Perform label encoding on the categorical features in the test data\n",
        "test_X_encoded = test_df.apply(lambda col: label_encoder_X.transform(col))\n",
        "\n",
        "# Make predictions on the test data\n",
        "test_y_pred = model.predict(test_X_encoded)\n",
        "\n",
        "# Convert the predicted labels back to their original classes for visualization\n",
        "predicted_classes = label_encoder_y.inverse_transform(test_y_pred)\n",
        "\n",
        "# Add the predicted classes to the test DataFrame\n",
        "test_df['Predicted_ESTIMATED_PHP_LOSS'] = predicted_classes\n",
        "\n",
        "test_df.head(10)\n"
      ],
      "metadata": {
        "id": "LhREwfsrs0m1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample test data for simulation (replace this with your actual test data)\n",
        "test_data = tempDf['FIRE_ORIGIN']\n",
        "\n",
        "# Convert the test data to DataFrame\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "# Perform label encoding on the categorical features in the test data\n",
        "test_X_encoded = test_df.apply(lambda col: label_encoder_X.transform(col))\n",
        "\n",
        "# Make predictions on the test data\n",
        "test_y_pred = model.predict(test_X_encoded)\n",
        "\n",
        "# Convert the predicted labels back to their original classes for visualization\n",
        "predicted_classes = label_encoder_y.inverse_transform(test_y_pred)\n",
        "\n",
        "# Add the predicted classes to the test DataFrame\n",
        "test_df['ESTIMATED_PHP_LOSS'] = predicted_classes\n",
        "\n",
        "# Get the count of 'low', 'medium', and 'high' for each unique value of 'FIRE_ORIGIN'\n",
        "result_df = test_df.groupby(['FIRE_ORIGIN', 'ESTIMATED_PHP_LOSS']).size().unstack(fill_value=0)\n",
        "\n",
        "# Add missing columns for 'medium' and 'high' and fill with 0 if not present\n",
        "result_df = result_df.reindex(columns=['low', 'medium', 'high'], fill_value=0)\n",
        "\n",
        "# Display the result\n",
        "print(result_df)\n"
      ],
      "metadata": {
        "id": "yAB0sMxQs2rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1jKnQIyVsjL"
      },
      "source": [
        "# Data Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wla6-9JMWiqp"
      },
      "source": [
        "# References:"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g-wG6a5PPAP7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}